{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsoTs6oG15J/lnhnB3VN6s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SwekeR-463/NLP-Basics/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27d0SEiY3mPZ",
        "outputId": "402f1229-8f97-406a-a30a-a4772a8ca76c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I love to eat Paneer Tikka Butter Masala!\""
      ],
      "metadata": {
        "id": "vx2k5lg13pcP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMQFffyb4lwk",
        "outputId": "e61ea440-1627-44ea-da0a-a121992fdcd5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "ekR2wL9o4u4f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE4CpnaR4zx3",
        "outputId": "6cfde582-873c-4786-c6dc-e1c7193c583d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'love', 'to', 'eat', 'Paneer', 'Tikka', 'Butter', 'Masala', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Character wise tokenization can be done using list()*"
      ],
      "metadata": {
        "id": "mNPMc3YG46sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char = list(sentence)"
      ],
      "metadata": {
        "id": "f60HIu-F43l_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ9xBfpq5IT-",
        "outputId": "c875de3d-3a4b-463f-c36b-81dfe4e97849"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " ' ',\n",
              " 'l',\n",
              " 'o',\n",
              " 'v',\n",
              " 'e',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'e',\n",
              " 'a',\n",
              " 't',\n",
              " ' ',\n",
              " 'P',\n",
              " 'a',\n",
              " 'n',\n",
              " 'e',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " 'T',\n",
              " 'i',\n",
              " 'k',\n",
              " 'k',\n",
              " 'a',\n",
              " ' ',\n",
              " 'B',\n",
              " 'u',\n",
              " 't',\n",
              " 't',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " 'M',\n",
              " 'a',\n",
              " 's',\n",
              " 'a',\n",
              " 'l',\n",
              " 'a',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Using wordpunct_tokenize*"
      ],
      "metadata": {
        "id": "YfIQ6Uai5Nkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "Cb1wzFNV5JBG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_punc = wordpunct_tokenize(sentence)"
      ],
      "metadata": {
        "id": "AciNQJW35Z7n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_punc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WOjK29y5eIn",
        "outputId": "ecf8f20f-4a09-4e8c-e4b5-63ab08ed0e45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'love', 'to', 'eat', 'Paneer', 'Tikka', 'Butter', 'Masala', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between wordpunct_tokenize() and word_tokenize() lies in how they handle punctuation marks.\n",
        "word_tokenize() ignores punctuation marks while wordpunct_tokenize() treats punctuation marks as separate tokens."
      ],
      "metadata": {
        "id": "-9fqEgyC5hir"
      }
    }
  ]
}